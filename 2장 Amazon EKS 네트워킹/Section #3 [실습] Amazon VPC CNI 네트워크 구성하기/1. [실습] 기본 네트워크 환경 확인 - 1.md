# 1. [실습] 기본 네트워크 환경 확인 - 1

   ## 1 기본 네트워크 환경 확인
   - Amazon EKS 원클릭 배포 환경에서 진행

   ### 1.1 기본 정보 확인
   - Amazon VPC CNI 구성 환경에서 기본적인 네트워크 정보를 확인
     
   #### 1.1.1 daemonset 확인
   ```
   // kube-system의 daemonset 확인
   kubectl get daemonset -n kube-system
   ```
   ![image](https://github.com/devhyunuk/eks-cloudnet/assets/49749510/e60b3b94-1d30-4fa7-bc07-00277a2f6445)

   1) VPC CNI 플러그인이 설치된 환경에서 노드별로 구성되는 데몬셋 정보를 확인
   2) 각각의 워커노드 3대에 aws-node와 kube-proxy가 데몬셋 형태로 기본 설치
   3) aws-node : VPC-CNI를 위한 컴포넌트
   4) kube-proxy : 노드에 실행되는 네트워킹 컴포넌트

   ```
   // vpc-cni 정보 확인
   kubectl describe daemonset aws-node -n kube-system | grep Image | cut -d "/" -f 2
   ```
   ![image](https://github.com/devhyunuk/eks-cloudnet/assets/49749510/f0a84e49-b19e-4e2f-901e-01e54265d497)
   - vpc cni 이미지 확인

   ```
   // kube-proxy config 확인
   kubectl describe cm -n kube-system kube-proxy-config
   ```
   ![image](https://github.com/devhyunuk/eks-cloudnet/assets/49749510/8499a65e-f2d6-4964-820b-7085c025f677)

   - 노드의 네트워킹 컴포넌트인 kubeproxy 설정 정보를 확인
   - kubeproxy가 동작하는 다양한 설정 값을 확인
   - vpc cni 환경은 ip tables 모드로 동작
   - 리눅스 커널에 구성되는 ip tables를 kubeproxy가 관리 제어
     
   #### 1.1.2 워커 노드 IP와 파드 IP 확인
   ```
   // 워커 노드 IP 확인
   aws ec2 describe-instances --query "Reservations[*].Instances[*].{PublicIPAdd:PublicIpAddress,PrivateIPAdd:PrivateIpAddress,InstanceName:Tags[?Key=='Name']|[0].Value,Status:State.Name}" --filters Name=instance-state-name,Values=running --output table
   ```
   ![image](https://github.com/devhyunuk/eks-cloudnet/assets/49749510/290aa1e8-547f-4c12-8425-717310c4c206)
   
   ```
   // 파드 IP 확인
   kubectl get pod -n kube-system -o=custom-columns=NAME:.metadata.name,IP:.status.podIP,STATUS:.status.phase
   ```
   ![image](https://github.com/devhyunuk/eks-cloudnet/assets/49749510/a1b18f24-772b-4dbb-b73b-10bec1373223)

   - aws-node pod가 워커노드 3대에 각각 설치
   - kube-proxy도 워커노드 3대에 각각 설치
   - aws-node와 kube-proxy는 워커노드의 Host Network Namespace에 위치해 인터페이스와 브리지 형태로 연결 (워커노드의 주소와 aws-node와 kube-proxy의 주소가 같은것을 확인 할 수 있다)
   - 즉, 노드의 IP와 Pod의 IP가 동일
   - 코어 DNS는 Kubernetes 클러스터 DNS 사용하는 서버
   - Amazon EKS 클러스터가 배포되면 노드 수에 상관없이 최초 두대의 coredns가 생성
   - coredns는 노드 IP와 다른 IP를 사용하는 것을 확인 할 수 있음 (Host Network Namespace가 아닌 파드 별 Network Namespace에 구성)
   - Pod IP는 vpc-cni를 통해 L-IPAM의 Warm Pool 대역에서 하나를 할당해 준다.
---
   
   ### 1.2 워커 노드에 사용할 도구 설치
   ```
   //  3대의 워커노드의 Private IP 주소 변수 저장
   N1=$(kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2a -o jsonpath={.items[0].status.addresses[0].address})
   N2=$(kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2b -o jsonpath={.items[0].status.addresses[0].address})
   N3=$(kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2c -o jsonpath={.items[0].status.addresses[0].address})
   echo "export N1=$N1" >> /etc/profile
   echo "export N2=$N2" >> /etc/profile
   echo "export N3=$N3" >> /etc/profile
   echo $N1, $N2, $N3
   ```
   ![image](https://github.com/devhyunuk/eks-cloudnet/assets/49749510/d75d1e5d-bac2-4ce6-b8d2-97e266ea879d)
   -  n1, n2, n3 변수에 WorkerNode Private IP 주소가 전역으로 선언되는 것을 확인 (/etc/profile에 저장)

   ```
   // 3대의 워커 노드에 사용할 도구 설치
   ssh ec2-user@$N1 sudo yum install links tree jq tcpdump -y
   
   ssh ec2-user@$N2 sudo yum install links tree jq tcpdump -y
   
   ssh ec2-user@$N3 sudo yum install links tree jq tcpdump -y
   ```
   ![image](https://github.com/devhyunuk/eks-cloudnet/assets/49749510/925ace3a-abe0-4542-9ec6-517779cc6b14)
   - 각각의 워커 노드에 설치

   
   #### 1.2.1 워커 노드에 네트워크 정보 확인
   ```
   // 워커 노드 1에서 vpc-cni 정보 확인
   ssh ec2-user@$N1 tree /var/log/aws-routed-eni
   
   ssh ec2-user@$N1 cat /var/log/aws-routed-eni/plugin.log | jq
   
   ssh ec2-user@$N1 cat /var/log/aws-routed-eni/ipamd.log | jq
   
   // 워커 노드 1에서 네트워크 정보 확인
   ssh ec2-user@$N1 sudo ip -br -c addr
   
   ssh ec2-user@$N1 sudo ip -c route
   
   // coreDNS 파드 정보 확인
   kubectl get pod -n kube-system -l k8s-app=kube-dns -owide
   ```
   ![image](https://github.com/devhyunuk/eks-cloudnet/assets/49749510/f549a371-ab7e-4578-8897-f2f0d528ffb3)
   [워커노드 1, 2]
   - t3.medium 인스턴스 유형은 ENI 당 6개의 slot이 존재할 수 있다.
   - 총 6개중에 첫번째 주소가 Primary 주소, 나머지 5개가 Secondary 주소 임.
   - Primary IP(프라이빗 IPv4주소)가 2개라는 것은 ENI가 2개라는 것을 뜻함.
   - ENI 당 5개의 Secondary IP를 가질 수 있어 총 10개의 Secondary IP(보조 프라이빗 IPv4 주소)를 사용할 수 있다

   ![image](https://github.com/devhyunuk/eks-cloudnet/assets/49749510/830fe4f9-c713-470f-beba-9eaa03dba486)
   [워커노드 3]
   - ENI 1
   - Primary 1개
   - Secondary 5개

   #### 최초 노드 3대 배포 시 구성 확인
   ![image](https://github.com/devhyunuk/eks-cloudnet/assets/49749510/32bc0b16-cf52-4421-8ccf-df93550a180a)
   1) 노드에는 Host Network Namespace가 구성되고 primary ENI인 eth0이 가상 라우터에 연결되어 있는 구조
   2) 특수용도의 파드인 aws-node와 kube-proxy가 구성
      - Host Network Namespace에 위치하고 eth0에 직접적인 연결
        
   3) primary ENI는 프라이빗 IP 주소를 획득 (첫번째 주소가 프라이머리 IP)
      - 나머지 5개는 세컨더리 IP 주소
   4) t3.medium은 ENI당 6개의 슬롯을 가질 수 있음 (현재 ENI의 최대 수량 6개 IP 주소를 보유)
      - 이런 IP들은 aws-node에 위치한 VPC-CNI의 L-IPAM의 warm pool 대역에서 할당한다.
   5) 특징 : aws-node와 kube-proxy는 eth0과 연결되어 프라이머리 IP 주소를 그대로 사용해서 같다.
   6) 클러스터가 배포되면 coredns 2대가 생성
      - 노드1과 노드2의 파드 형태로 별도의 Per Pod Network Namespace에 배치된다.
      - 통신을 위해 노드의 가상의 인터페이스(그림의 eni666, eni711)와 페어링되어 라우터에 연결
   7) Primary ENI와 마찬가지로 Secondary ENI도 최대 수량의 IP 주소를 미리 할당/확보해 둔다.
      - coreDNS 파드는 현재 노드가 보유한 Secondary IP 중에 하나를 할당받는다. (Primary ENI의 6개가 부족하기 때문에 > Secondary ENI를 생성하여 coreDNS에 할당한다.)
---

















