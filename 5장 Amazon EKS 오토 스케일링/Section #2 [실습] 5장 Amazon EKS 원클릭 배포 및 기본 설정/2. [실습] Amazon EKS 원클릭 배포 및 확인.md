# 2. [실습] Amazon EKS 원클릭 배포 및 확인

![image](https://github.com/devhyunuk/eks-cloudnet/assets/49749510/1a05a7c7-7e4e-4d8a-b77c-6f19e5134ac4)


# 1. Amazon EKS 원클릭 배포
- 5장 기본 인프라 환경과 Amazon EKS 클러스터 배포에 대해 CloudFormation을 통해 원클릭 배포를 수행.

## 1.1. CloudFormation 스택 생성
- eks-oneclick4.yaml를 활용하여 파라미터를 입력 후 스택을 생성
- Note: AWS 관리 콘솔에 로그인 할때 IAM 사용자 계정으로 진행

### 1.1.1 [관리 콘솔] CloudFormation 파라미터 설정
- 스택 생성 페이지에서 다음 버튼을 클릭
- 스택 이름은 [myeks]로 입력
- KeyName은 [각자의 키페어]를 선택
- MyIAMUserAccessKeyID는 [각자의 액세스 키 ID 값]을 입력
- MyIAMUserSecretAccessKey는 [각자의 시크릿 액세스 키 값]을 입력
- SgIngressSshCidr는 [각자의 PC의 퍼블릭 IP 주소/32]로 입력
- 나머지 파라미터는 기본 값을 유지하고 다음 버튼을 클릭
- Warning: 설정을 마치고 약 20분 정도 대기 시간이 흐른 뒤 기본 인프라 환경과 Amazon EKS 클러스터 생성이 완료. 반드시 해당 대기 시간이 지난 후 다음 작업을 수행.

## 1.2. CloudFormation 스택 생성 확인
- Amazon EKS 원클릭 배포를 수행하면 AWS CloudFormation 스택 9개가 생성

### 1.2.1 CloudFormation 스택 정보
- myeks: 기본 인프라 생성을 정의한 스택
- eksctl-myeks-cluster: eks 클러스터 생성을 정의한 스택
- eksctl-myeks-addon-vpc-cni: vpc cni를 위한 IAM 역할을 정의한 스택
- eksctl-myeks-nodegroup-ng1: eks 클러스터의 관리형 노드 그룹을 정의한 스택
- eksctl-myeks-addon-iamserviceaccount-kube-system-aws-load-balancer-controller: aws load balancer controller를 위한 IRSA를 정의한 스택
- eksctl-myeks-addon-ebs-csi-driver: ebs-csi-driver를 정의한 스택
- eksctl-myeks-addon-efs-csi-driver: efs-csi-driver를 정의한 스택
- eksctl-myeks-addon-iamserviceaccount-kube-system-ebs-csi-controller-sa: ebs-csi-controller를 위한 IRSA를 정의한 스택
- eksctl-myeks-addon-iamserviceaccount-kube-system-efs-csi-controller-sa: efs-csi-controller를 위한 IRSA를 정의한 스택
- Note: myeks 스택의 출력 탭에서 작업용 인스턴스의 퍼블릭 IP 주소를 확인 가능.

---

# 2. Amazon EKS 원클릭 배포 정보 확인
- AWS CloudFormation 스택의 출력 탭에서 eksctlhost의 퍼블릭 IP를 확인
- 해당 IP로 EKS 관리용 인스턴스(myeks-bastion-EC2)에 SSH로 접속하고 아래 명령어를 통해 정보를 확인

## 2.1. 기본 정보 확인
- Amazon EKS 원클릭으로 배포된 기본 정보와 설정을 진행

### 2.1.1 Default Namespace로 적용
```
// Default Namespace로 위치 변경
kubectl ns default
```

### 2.1.2 노드의 프라이빗 IP 변수 선언
```
//  3대의 워커노드의 Private IP 주소 변수 저장
N1=$(kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2a -o jsonpath={.items[0].status.addresses[0].address})

N2=$(kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2b -o jsonpath={.items[0].status.addresses[0].address})

N3=$(kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2c -o jsonpath={.items[0].status.addresses[0].address})

// 3대의 워커노드의 Private IP 주소 전역 변수로 선언 및 확인
echo "export N1=$N1" >> /etc/profile

echo "export N2=$N2" >> /etc/profile

echo "export N3=$N3" >> /etc/profile

echo $N1, $N2, $N3
```

### 2.1.3 작업용 인스턴스에서 노드로 보안 그룹 설정
```
// 노드 보안 그룹 ID를 변수 선언
NGSGID=$(aws ec2 describe-security-groups --filters Name=group-name,Values=*ng1* --query "SecurityGroups[*].[GroupId]" --output text)

// 노드 보안 그룹에 정책 추가 - 작업용 인스턴스에서 노드로 모든 통신 허용
aws ec2 authorize-security-group-ingress --group-id $NGSGID --protocol '-1' --cidr 192.168.1.100/32
```

### 2.1.4 워커 노드 SSH 접근 확인
```
// N1, N2, N3 워커 노드에 SSH 접속 후 hostname 명령어 수행
for node in $N1 $N2 $N3; do ssh ec2-user@$node hostname; done
```

## 2.2. 기본 설정 작업
- 5장 다양한 도구 설치와 기본 설정을 수행

### 2.2.1 AWS Load Balancer Controller 설치
```
helm repo add eks https://aws.github.io/eks-charts

helm repo update

helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=$CLUSTER_NAME \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller
```

### 2.2.2 ExternalDNS 설치
```
MyDomain=<자신의 도메인>

MyDnsHostedZoneId=$(aws route53 list-hosted-zones-by-name --dns-name "${MyDomain}." --query "HostedZones[0].Id" --output text)

echo $MyDomain, $MyDnsHostedZoneId

curl -s -O https://raw.githubusercontent.com/cloudneta/cnaeblab/master/_data/externaldns.yaml

MyDomain=$MyDomain MyDnsHostedZoneId=$MyDnsHostedZoneId envsubst < externaldns.yaml | kubectl apply -f -
```

### 2.2.3 kube-ops-view 설치
```
helm repo add geek-cookbook https://geek-cookbook.github.io/charts/

helm install kube-ops-view geek-cookbook/kube-ops-view --version 1.2.2 --set env.TZ="Asia/Seoul" --namespace kube-system

kubectl patch svc -n kube-system kube-ops-view -p '{"spec":{"type":"LoadBalancer"}}'

kubectl annotate service kube-ops-view -n kube-system "external-dns.alpha.kubernetes.io/hostname=kubeopsview.$MyDomain"

echo -e "Kube Ops View URL = http://kubeopsview.$MyDomain:8080/#scale=1.5"
```

### 2.2.4 AWS Cert Manager 인증서 변수화
```
// ACM 인증서 변수 선언
CERT_ARN=`aws acm list-certificates --query 'CertificateSummaryList[].CertificateArn[]' --output text`; echo $CERT_ARN
```

---

### 2.2.5 프로메테우스 스택 설치
1) 오토스케일링 확인시 스케일링 동작 확인을 위해 프로메테우스와 그라파나를 활용 -> 프로메테우스 스택으로 관련 도구를 설치 
```
// monitoring 네임 스페이스 생성
kubectl create ns monitoring
```
![image](https://github.com/devhyunuk/eks-cloudnet/assets/49749510/2ba245b6-8760-40a6-b593-29bdcc0b21b6)
1) 프로메테우스 자원이 위치할 모니터링 네임스페이스를 생성
   
```
// helm chart repository 추가
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
```
![image](https://github.com/devhyunuk/eks-cloudnet/assets/49749510/da15483a-8ff6-43ef-b37f-a8b9b9d1b8a6)

```
// 프로메테우스 스택 파라미터 구성
cat <<EOT > monitor-values.yaml
prometheus:
  prometheusSpec:
    podMonitorSelectorNilUsesHelmValues: false
    serviceMonitorSelectorNilUsesHelmValues: false
    retention: 5d
    retentionSize: "10GiB"

  verticalPodAutoscaler:
    enabled: true

  ingress:
    enabled: true
    ingressClassName: alb
    hosts: 
      - prometheus.$MyDomain
    paths: 
      - /*
    annotations:
      alb.ingress.kubernetes.io/scheme: internet-facing
      alb.ingress.kubernetes.io/target-type: ip
      alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS":443}, {"HTTP":80}]'
      alb.ingress.kubernetes.io/certificate-arn: $CERT_ARN
      alb.ingress.kubernetes.io/success-codes: 200-399
      alb.ingress.kubernetes.io/load-balancer-name: myeks-ingress-alb
      alb.ingress.kubernetes.io/group.name: study
      alb.ingress.kubernetes.io/ssl-redirect: '443'

grafana:
  defaultDashboardsTimezone: Asia/Seoul
  adminPassword: prom-operator

  ingress:
    enabled: true
    ingressClassName: alb
    hosts: 
      - grafana.$MyDomain
    paths: 
      - /*
    annotations:
      alb.ingress.kubernetes.io/scheme: internet-facing
      alb.ingress.kubernetes.io/target-type: ip
      alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS":443}, {"HTTP":80}]'
      alb.ingress.kubernetes.io/certificate-arn: $CERT_ARN
      alb.ingress.kubernetes.io/success-codes: 200-399
      alb.ingress.kubernetes.io/load-balancer-name: myeks-ingress-alb
      alb.ingress.kubernetes.io/group.name: study
      alb.ingress.kubernetes.io/ssl-redirect: '443'

defaultRules:
  create: false
kubeControllerManager:
  enabled: false
kubeEtcd:
  enabled: false
kubeScheduler:
  enabled: false
alertmanager:
  enabled: false
EOT
```
![image](https://github.com/devhyunuk/eks-cloudnet/assets/49749510/dc4e54b9-ae6d-439b-8323-739265aa74be)

```
// 프로메테우스 스택 배포
helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack --version 45.27.2 \
  --set prometheus.prometheusSpec.scrapeInterval='15s' \
  --set prometheus.prometheusSpec.evaluationInterval='15s' \
  -f monitor-values.yaml --namespace monitoring
```
![image](https://github.com/devhyunuk/eks-cloudnet/assets/49749510/fe294eb3-411e-4087-b619-6306d22d7e9b)
1) 해당 파라미터 파일을 활용하여 프로메테우스 스택을 배포 시작.

![image](https://github.com/devhyunuk/eks-cloudnet/assets/49749510/78a26f5b-d0a3-4312-9973-a72d808b5c12)
1) 스택 설치 완료 확인
![image](https://github.com/devhyunuk/eks-cloudnet/assets/49749510/fdfbbe74-cf3e-4c81-a3db-87504fbc9a49)
1) 프로메테우스 웹 접속 확인
![image](https://github.com/devhyunuk/eks-cloudnet/assets/49749510/eda5d6af-1e47-456c-bea8-bb69e0a707ee)
1) 그라파나 웹 접속 확인

---

### 2.2.6 메트릭 서버 설치
1) HPA와 VPA를 위한 매트릭 서버 설치 시작.

![image](https://github.com/devhyunuk/eks-cloudnet/assets/49749510/e7511c17-b076-4fcd-bb69-37ee5837b0f9)
1) **매트릭 서버**를 배포하면 kubelet으로부터 파드나 노드에 대한 매트릭을 집계할 수 있기 때문에 설치 시작.

```
// 메트릭 서버 배포
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```
![image](https://github.com/devhyunuk/eks-cloudnet/assets/49749510/7a0a4505-dc6d-4648-a2bf-617fe1ef9191)
![image](https://github.com/devhyunuk/eks-cloudnet/assets/49749510/265af17f-dc7f-46c8-a4ae-72daab9be2a4)
1) kube-system 네임 스페이스에 매트릭 서버가 생성된것 확인.
   
```
// 메트릭 서버 Pod를 확인한다. : 메트릭은 15초 간격으로 cAdvisor를 통하여 가져옴
kubectl get pod -n kube-system -l k8s-app=metrics-server
```
![image](https://github.com/devhyunuk/eks-cloudnet/assets/49749510/f4b9707f-9991-4731-a564-6b38cbd9cf2d)
1) 명령어를 통해 매트릭 서버가 생성 확인.
   
```
kubectl api-resources | grep metrics
```
![image](https://github.com/devhyunuk/eks-cloudnet/assets/49749510/9965914f-fbe5-4bc5-a073-93ab2e5a4769)
1) 매트릭 서버가 수집한 정보는 api 형태로 요청하고 받아올 수 있음.
2) 명령어 kubectl api-resource 명령으로 매트릭만 필터링해서 확인해 본다.
   - 노드에 대한 노드 매트릭
   - 파드에 대한 파드 매트릭
   - 의 api 자원이 존재하는 것을 확인 완료.
  
3) 위 두가지 매트릭 서버를 통해 hpa나 vpa가 정보를 호출해서 사용한다.
```
// 노드 메트릭 확인
kubectl top node
```
![image](https://github.com/devhyunuk/eks-cloudnet/assets/49749510/7804ac85-311a-4afe-9c89-7d9cb1eaa17e)
1) kubectl top 명령으로 노드의 매트릭을 확인
   - 3대의 노드에 대한 매트릭을 수집해서 아래 확인 가능.
     - CPU의 코어 사용률
     - CPU 사용률
     - 메모리 용량
```
// 파드 메트릭 확인
kubectl top pod -A
```
![image](https://github.com/devhyunuk/eks-cloudnet/assets/49749510/7d9692be-7fea-4b01-8b8f-9c04a9272c03)
1) EKS 클러스터에 존재하는 모든 파드에 대한 CPU 코어와 메모리를 확인할 수 있음.

```
kubectl top pod -n kube-system --sort-by='cpu'
```
![image](https://github.com/devhyunuk/eks-cloudnet/assets/49749510/a421f8b6-0d4c-44cc-814b-9905082cc193)
1) 명령을 통해 높은 값부터 정렬해서 볼수가 있음.

```
kubectl top pod -n kube-system --sort-by='memory'
```

### 2.2.7 EKS Node Viewer 설치
1) EKS 노드 뷰어라는 도구를 설치를 통해
   - 터미널 상에서 노드의 가용 용량과 리소스 사용량을 가시성 있게 확인 가능.
   - EKS Node Viewer는 실제 파드의 사용량을 측정하는 것은 아님.
     - 대상으로 요청하는 리소스만 측정되기 때문에 정확한 측정치는 아니고 참고 용도로 활용한다.
```
// go 설치
yum install -y go
```
![image](https://github.com/devhyunuk/eks-cloudnet/assets/49749510/6ace4bd0-5cdb-4b33-9160-76ad907d05d2)
1) EKS Node Viewer는 go 프로그램 언어로 구성되어 Go를 설치한 후 명령어를 수행한다.
```
// EKS Node Viewer 설치 (2분 이상 소요)
go install github.com/awslabs/eks-node-viewer/cmd/eks-node-viewer@v0.5.0
```
![image](https://github.com/devhyunuk/eks-cloudnet/assets/49749510/e5353b7c-150b-45f3-9434-e886c8e99c0a)

```
// 신규 터미널에서 실행
cd ~/go/bin

./eks-node-viewer
```
![image](https://github.com/devhyunuk/eks-cloudnet/assets/49749510/f322386c-9306-44fc-983c-7b9dfbf718f6)
1) 3대의 노드에 대한 CPU 사용률과 구성된 파드 수량이 보임.
2) 인스턴스 유형과 시간당 금액도 확인 가능.
3) 가장 상단의 전체 노드에 CPU 평균값도 출력 확인 가능.

---

## 3. 실습 환경 삭제
- Amazon EKS 원클릭을 통해 배포된 모든 자원을 삭제.

### 3.1 helm chart 삭제
```
helm uninstall -n monitoring kube-prometheus-stack

helm uninstall -n kube-system kube-ops-view
```

### 3.2 Amazon EKS 원클릭 배포의 삭제
```
// Amazon EKS 원클릭 배포의 삭제
eksctl delete cluster --name $CLUSTER_NAME \
  && aws cloudformation delete-stack --stack-name $CLUSTER_NAME
Warning: Amazon EKS 원클릭 배포의 삭제는 약 15분 정도 소요됩니다. 삭제가 완료될 때 까지 SSH 연결 세션을 유지합니다.
```
- Warning: 만약에 CloudFormation 스택이 삭제되지 않는다면 수동으로 VPC(myeks-VPC)를 삭제 후 CloudFormation 스택을 다시 삭제.
































